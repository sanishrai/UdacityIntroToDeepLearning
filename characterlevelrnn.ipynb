{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:15:28.067270Z","iopub.execute_input":"2021-05-24T22:15:28.067592Z","iopub.status.idle":"2021-05-24T22:15:28.071967Z","shell.execute_reply.started":"2021-05-24T22:15:28.067562Z","shell.execute_reply":"2021-05-24T22:15:28.070752Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Main solution: https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/Character_Level_RNN_Solution.ipynb","metadata":{}},{"cell_type":"code","source":"with open('../input/anna-karenina-book/anna.txt','r') as f:\n    text = f.read()","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:15:30.171274Z","iopub.execute_input":"2021-05-24T22:15:30.171634Z","iopub.status.idle":"2021-05-24T22:15:30.245382Z","shell.execute_reply.started":"2021-05-24T22:15:30.171600Z","shell.execute_reply":"2021-05-24T22:15:30.244460Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"text[:100]","metadata":{"execution":{"iopub.status.busy":"2021-05-24T00:13:14.391846Z","iopub.execute_input":"2021-05-24T00:13:14.392194Z","iopub.status.idle":"2021-05-24T00:13:14.400041Z","shell.execute_reply.started":"2021-05-24T00:13:14.392163Z","shell.execute_reply":"2021-05-24T00:13:14.398927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tokenization\nchars = tuple(set(text))\nint2char=dict(enumerate(chars))\n#print(int2char)\nchar2int={ch:ii for ii,ch in int2char.items()}\n#print(\"\\n\",char2int)\n\n#encode the text\nencoded = np.array([char2int[ch] for ch in text])\nprint(encoded.size)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:15:34.281730Z","iopub.execute_input":"2021-05-24T22:15:34.282084Z","iopub.status.idle":"2021-05-24T22:15:34.720523Z","shell.execute_reply.started":"2021-05-24T22:15:34.282055Z","shell.execute_reply":"2021-05-24T22:15:34.719621Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"1985223\n","output_type":"stream"}]},{"cell_type":"code","source":"def one_hot_encode(arr,n_labels):\n    one_hot = np.zeros((arr.size,n_labels),dtype=np.float32)\n    one_hot[np.arange(one_hot.shape[0]),arr.flatten()]=1\n    one_hot = one_hot.reshape((*arr.shape,n_labels))\n    return one_hot","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:15:37.462744Z","iopub.execute_input":"2021-05-24T22:15:37.465101Z","iopub.status.idle":"2021-05-24T22:15:37.474924Z","shell.execute_reply.started":"2021-05-24T22:15:37.465056Z","shell.execute_reply":"2021-05-24T22:15:37.474188Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# check that the function works as expected\ntest_seq = np.array([[3, 5, 1]])\none_hot = one_hot_encode(test_seq, 8)\n\nprint(one_hot)","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:15:39.824893Z","iopub.execute_input":"2021-05-24T22:15:39.825209Z","iopub.status.idle":"2021-05-24T22:15:39.830786Z","shell.execute_reply.started":"2021-05-24T22:15:39.825181Z","shell.execute_reply":"2021-05-24T22:15:39.829949Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"[[[0. 0. 0. 1. 0. 0. 0. 0.]\n  [0. 0. 0. 0. 0. 1. 0. 0.]\n  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_batches(arr,batch_size,seq_length):\n    batch_size_total = batch_size*seq_length\n    n_batches=len(arr)//batch_size_total\n    \n    arr = arr[:n_batches*batch_size_total]\n    arr = arr.reshape(batch_size,-1)\n    \n    for n in range(0,arr.shape[1],seq_length):\n        x = arr[:,n:n+seq_length]\n        y=np.zeros_like(x)\n        try:\n            y[:,:-1],y[:,-1]=x[:,1:], arr[:,n+seq_length]\n        except IndexError:\n            y[:,:-1],y[:,-1] = x[:,1:],arr[:,0]\n        yield x,y\n        \n    ","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:15:42.282565Z","iopub.execute_input":"2021-05-24T22:15:42.282894Z","iopub.status.idle":"2021-05-24T22:15:42.290452Z","shell.execute_reply.started":"2021-05-24T22:15:42.282862Z","shell.execute_reply":"2021-05-24T22:15:42.289617Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"batches = get_batches(encoded,8,4)\nx,y = next(batches)\n# print('x\\n',x[:10,:10])\n# print('y\\n',y[:10,:10])","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:15:45.105738Z","iopub.execute_input":"2021-05-24T22:15:45.106143Z","iopub.status.idle":"2021-05-24T22:15:45.110373Z","shell.execute_reply.started":"2021-05-24T22:15:45.106109Z","shell.execute_reply":"2021-05-24T22:15:45.109364Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# check if GPU is available\ntrain_on_gpu = torch.cuda.is_available()\nif(train_on_gpu):\n    print('Training on GPU!')\nelse: \n    print('No GPU available, training on CPU; consider making n_epochs very small.')\n","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:15:47.913374Z","iopub.execute_input":"2021-05-24T22:15:47.913709Z","iopub.status.idle":"2021-05-24T22:15:47.995917Z","shell.execute_reply.started":"2021-05-24T22:15:47.913677Z","shell.execute_reply":"2021-05-24T22:15:47.993907Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Training on GPU!\n","output_type":"stream"}]},{"cell_type":"code","source":"class CharRNN(nn.Module):\n    def __init__(self, tokens, n_hidden=256,n_layers=2,\n                drop_prob=0.5, lr=0.001):\n        super().__init__()\n        self.drop_prob=drop_prob\n        self.n_layers = n_layers\n        self.n_hidden = n_hidden\n        self.lr = lr\n        \n        #creating char dictionaries\n        self.chars = tokens\n        self.int2char = dict(enumerate(self.chars))\n        self.char2int = {ch:ii for ii,ch in self.int2char.items()}\n        \n        #lstm layer\n        input_size=len(self.chars)\n        self.lstm = nn.LSTM(input_size,n_hidden,n_layers,\n                           dropout=drop_prob, batch_first=True)\n        \n        #dropout layer\n        self.dropout = nn.Dropout(drop_prob)\n        self.fc = nn.Linear(n_hidden,len(self.chars))\n        \n    def forward(self,x,hidden):\n        \n        r_output, hidden = self.lstm(x,hidden)\n        out = self.dropout(r_output)\n         # Stack up LSTM outputs using view\n        # you may need to use contiguous to reshape the output\n        out = out.contiguous().view(-1,self.n_hidden)\n        #put x through the fully connected layer\n        out = self.fc(out)\n        \n        return out, hidden\n    \n    def init_hidden(self, batch_size):\n        '''' Initializes hidden state '''\n        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n        # initialized to zero, for hidden state and cell state of LSTM\n        weight = next(self.parameters()).data\n        if (train_on_gpu):\n            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n        else:\n            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n        return hidden","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:15:49.848680Z","iopub.execute_input":"2021-05-24T22:15:49.849044Z","iopub.status.idle":"2021-05-24T22:15:49.860615Z","shell.execute_reply.started":"2021-05-24T22:15:49.849007Z","shell.execute_reply":"2021-05-24T22:15:49.859377Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n    net.train()\n    opt = torch.optim.Adam(net.parameters(),lr=lr)\n    criterion = nn.CrossEntropyLoss()\n    \n    #create training and validation data\n    val_idx = int(len(data)*(1-val_frac))\n    data,val_data = data[:val_idx],data[val_idx:]\n    \n    if(train_on_gpu):\n        net.cuda()\n    counter =0\n    n_chars = len(net.chars)\n    for e in range(epochs):\n        h = net.init_hidden(batch_size)\n        \n        for x,y in get_batches(data,batch_size,seq_length):\n            counter+=1\n            x=one_hot_encode(x,n_chars)\n            inputs,targets = torch.from_numpy(x), torch.from_numpy(y)\n            if(train_on_gpu):\n                inputs,targets=inputs.cuda(),targets.cuda()\n            # Creating new variables for the hidden state, otherwise\n            # we'd backprop through the entire training history\n            h=tuple([each.data for each in h])\n            \n            net.zero_grad()\n            output,h=net(inputs,h)\n            loss = criterion(output,targets.view(batch_size*seq_length).long())\n            loss.backward()\n            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n            nn.utils.clip_grad_norm_(net.parameters(), clip)\n            opt.step()\n            \n             # loss stats\n            if counter % print_every == 0:\n                # Get validation loss\n                val_h = net.init_hidden(batch_size)\n                val_losses = []\n                net.eval()\n                for x, y in get_batches(val_data, batch_size, seq_length):\n                    # One-hot encode our data and make them Torch tensors\n                    x = one_hot_encode(x, n_chars)\n                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n                    \n                    # Creating new variables for the hidden state, otherwise\n                    # we'd backprop through the entire training history\n                    val_h = tuple([each.data for each in val_h])\n                    \n                    inputs, targets = x, y\n                    if(train_on_gpu):\n                        inputs, targets = inputs.cuda(), targets.cuda()\n\n                    output, val_h = net(inputs, val_h)\n                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n                \n                    val_losses.append(val_loss.item())\n                \n                net.train() # reset to train mode after iterationg through validation data\n                \n                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n                      \"Step: {}...\".format(counter),\n                      \"Loss: {:.4f}...\".format(loss.item()),\n                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n    ","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:15:52.389337Z","iopub.execute_input":"2021-05-24T22:15:52.389774Z","iopub.status.idle":"2021-05-24T22:15:52.417972Z","shell.execute_reply.started":"2021-05-24T22:15:52.389733Z","shell.execute_reply":"2021-05-24T22:15:52.416860Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#model hyperparameters\nn_hidden=128\nn_layers=2\nnet=CharRNN(chars,n_hidden,n_layers)\nprint(net)","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:15:55.567062Z","iopub.execute_input":"2021-05-24T22:15:55.567375Z","iopub.status.idle":"2021-05-24T22:15:55.592373Z","shell.execute_reply.started":"2021-05-24T22:15:55.567346Z","shell.execute_reply":"2021-05-24T22:15:55.591336Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"CharRNN(\n  (lstm): LSTM(83, 128, num_layers=2, batch_first=True, dropout=0.5)\n  (dropout): Dropout(p=0.5, inplace=False)\n  (fc): Linear(in_features=128, out_features=83, bias=True)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"#training hyperparameters\nbatch_size = 128\nseq_length = 100\nn_epochs=30\ntrain(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)\n","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:17:39.883778Z","iopub.execute_input":"2021-05-24T22:17:39.884128Z","iopub.status.idle":"2021-05-24T22:19:41.561829Z","shell.execute_reply.started":"2021-05-24T22:17:39.884099Z","shell.execute_reply":"2021-05-24T22:19:41.560984Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Epoch: 1/30... Step: 10... Loss: 3.1233... Val Loss: 3.1084\nEpoch: 1/30... Step: 20... Loss: 3.0891... Val Loss: 3.1040\nEpoch: 1/30... Step: 30... Loss: 3.1036... Val Loss: 3.0975\nEpoch: 1/30... Step: 40... Loss: 3.0754... Val Loss: 3.0879\nEpoch: 1/30... Step: 50... Loss: 3.0994... Val Loss: 3.0721\nEpoch: 1/30... Step: 60... Loss: 3.0515... Val Loss: 3.0455\nEpoch: 1/30... Step: 70... Loss: 3.0109... Val Loss: 3.0026\nEpoch: 1/30... Step: 80... Loss: 2.9825... Val Loss: 2.9520\nEpoch: 1/30... Step: 90... Loss: 2.9348... Val Loss: 2.8905\nEpoch: 1/30... Step: 100... Loss: 2.8737... Val Loss: 2.8387\nEpoch: 1/30... Step: 110... Loss: 2.8365... Val Loss: 2.7864\nEpoch: 1/30... Step: 120... Loss: 2.7579... Val Loss: 2.7354\nEpoch: 1/30... Step: 130... Loss: 2.7479... Val Loss: 2.6947\nEpoch: 2/30... Step: 140... Loss: 2.7149... Val Loss: 2.6520\nEpoch: 2/30... Step: 150... Loss: 2.6858... Val Loss: 2.6141\nEpoch: 2/30... Step: 160... Loss: 2.6333... Val Loss: 2.5864\nEpoch: 2/30... Step: 170... Loss: 2.5859... Val Loss: 2.5636\nEpoch: 2/30... Step: 180... Loss: 2.5750... Val Loss: 2.5424\nEpoch: 2/30... Step: 190... Loss: 2.5496... Val Loss: 2.5168\nEpoch: 2/30... Step: 200... Loss: 2.5490... Val Loss: 2.4991\nEpoch: 2/30... Step: 210... Loss: 2.5330... Val Loss: 2.4815\nEpoch: 2/30... Step: 220... Loss: 2.5210... Val Loss: 2.4655\nEpoch: 2/30... Step: 230... Loss: 2.4989... Val Loss: 2.4480\nEpoch: 2/30... Step: 240... Loss: 2.4995... Val Loss: 2.4325\nEpoch: 2/30... Step: 250... Loss: 2.4550... Val Loss: 2.4208\nEpoch: 2/30... Step: 260... Loss: 2.4452... Val Loss: 2.4080\nEpoch: 2/30... Step: 270... Loss: 2.4414... Val Loss: 2.3929\nEpoch: 3/30... Step: 280... Loss: 2.4537... Val Loss: 2.3835\nEpoch: 3/30... Step: 290... Loss: 2.4340... Val Loss: 2.3711\nEpoch: 3/30... Step: 300... Loss: 2.4226... Val Loss: 2.3628\nEpoch: 3/30... Step: 310... Loss: 2.4151... Val Loss: 2.3556\nEpoch: 3/30... Step: 320... Loss: 2.4043... Val Loss: 2.3436\nEpoch: 3/30... Step: 330... Loss: 2.3782... Val Loss: 2.3341\nEpoch: 3/30... Step: 340... Loss: 2.3928... Val Loss: 2.3250\nEpoch: 3/30... Step: 350... Loss: 2.3852... Val Loss: 2.3151\nEpoch: 3/30... Step: 360... Loss: 2.3340... Val Loss: 2.3049\nEpoch: 3/30... Step: 370... Loss: 2.3736... Val Loss: 2.2949\nEpoch: 3/30... Step: 380... Loss: 2.3507... Val Loss: 2.2848\nEpoch: 3/30... Step: 390... Loss: 2.3357... Val Loss: 2.2777\nEpoch: 3/30... Step: 400... Loss: 2.3171... Val Loss: 2.2671\nEpoch: 3/30... Step: 410... Loss: 2.3364... Val Loss: 2.2578\nEpoch: 4/30... Step: 420... Loss: 2.3053... Val Loss: 2.2489\nEpoch: 4/30... Step: 430... Loss: 2.3051... Val Loss: 2.2423\nEpoch: 4/30... Step: 440... Loss: 2.3014... Val Loss: 2.2323\nEpoch: 4/30... Step: 450... Loss: 2.2523... Val Loss: 2.2241\nEpoch: 4/30... Step: 460... Loss: 2.2699... Val Loss: 2.2165\nEpoch: 4/30... Step: 470... Loss: 2.2718... Val Loss: 2.2085\nEpoch: 4/30... Step: 480... Loss: 2.2522... Val Loss: 2.1984\nEpoch: 4/30... Step: 490... Loss: 2.2585... Val Loss: 2.1925\nEpoch: 4/30... Step: 500... Loss: 2.2536... Val Loss: 2.1837\nEpoch: 4/30... Step: 510... Loss: 2.2622... Val Loss: 2.1788\nEpoch: 4/30... Step: 520... Loss: 2.2684... Val Loss: 2.1719\nEpoch: 4/30... Step: 530... Loss: 2.2240... Val Loss: 2.1665\nEpoch: 4/30... Step: 540... Loss: 2.2041... Val Loss: 2.1542\nEpoch: 4/30... Step: 550... Loss: 2.2277... Val Loss: 2.1482\nEpoch: 5/30... Step: 560... Loss: 2.2051... Val Loss: 2.1416\nEpoch: 5/30... Step: 570... Loss: 2.1999... Val Loss: 2.1373\nEpoch: 5/30... Step: 580... Loss: 2.1845... Val Loss: 2.1298\nEpoch: 5/30... Step: 590... Loss: 2.1999... Val Loss: 2.1196\nEpoch: 5/30... Step: 600... Loss: 2.1598... Val Loss: 2.1153\nEpoch: 5/30... Step: 610... Loss: 2.1742... Val Loss: 2.1082\nEpoch: 5/30... Step: 620... Loss: 2.1575... Val Loss: 2.1012\nEpoch: 5/30... Step: 630... Loss: 2.1897... Val Loss: 2.0951\nEpoch: 5/30... Step: 640... Loss: 2.1676... Val Loss: 2.0895\nEpoch: 5/30... Step: 650... Loss: 2.1527... Val Loss: 2.0837\nEpoch: 5/30... Step: 660... Loss: 2.1171... Val Loss: 2.0785\nEpoch: 5/30... Step: 670... Loss: 2.1721... Val Loss: 2.0747\nEpoch: 5/30... Step: 680... Loss: 2.1531... Val Loss: 2.0662\nEpoch: 5/30... Step: 690... Loss: 2.1240... Val Loss: 2.0612\nEpoch: 6/30... Step: 700... Loss: 2.1131... Val Loss: 2.0566\nEpoch: 6/30... Step: 710... Loss: 2.1223... Val Loss: 2.0488\nEpoch: 6/30... Step: 720... Loss: 2.0994... Val Loss: 2.0434\nEpoch: 6/30... Step: 730... Loss: 2.1105... Val Loss: 2.0390\nEpoch: 6/30... Step: 740... Loss: 2.1062... Val Loss: 2.0352\nEpoch: 6/30... Step: 750... Loss: 2.0665... Val Loss: 2.0309\nEpoch: 6/30... Step: 760... Loss: 2.1041... Val Loss: 2.0244\nEpoch: 6/30... Step: 770... Loss: 2.0950... Val Loss: 2.0197\nEpoch: 6/30... Step: 780... Loss: 2.0798... Val Loss: 2.0131\nEpoch: 6/30... Step: 790... Loss: 2.0867... Val Loss: 2.0093\nEpoch: 6/30... Step: 800... Loss: 2.0808... Val Loss: 2.0074\nEpoch: 6/30... Step: 810... Loss: 2.0826... Val Loss: 2.0025\nEpoch: 6/30... Step: 820... Loss: 2.0603... Val Loss: 1.9968\nEpoch: 6/30... Step: 830... Loss: 2.0818... Val Loss: 1.9953\nEpoch: 7/30... Step: 840... Loss: 2.0462... Val Loss: 1.9881\nEpoch: 7/30... Step: 850... Loss: 2.0471... Val Loss: 1.9841\nEpoch: 7/30... Step: 860... Loss: 2.0491... Val Loss: 1.9792\nEpoch: 7/30... Step: 870... Loss: 2.0564... Val Loss: 1.9734\nEpoch: 7/30... Step: 880... Loss: 2.0528... Val Loss: 1.9704\nEpoch: 7/30... Step: 890... Loss: 2.0416... Val Loss: 1.9682\nEpoch: 7/30... Step: 900... Loss: 2.0149... Val Loss: 1.9619\nEpoch: 7/30... Step: 910... Loss: 2.0201... Val Loss: 1.9584\nEpoch: 7/30... Step: 920... Loss: 2.0187... Val Loss: 1.9526\nEpoch: 7/30... Step: 930... Loss: 2.0167... Val Loss: 1.9504\nEpoch: 7/30... Step: 940... Loss: 2.0123... Val Loss: 1.9474\nEpoch: 7/30... Step: 950... Loss: 2.0343... Val Loss: 1.9446\nEpoch: 7/30... Step: 960... Loss: 2.0241... Val Loss: 1.9398\nEpoch: 7/30... Step: 970... Loss: 2.0267... Val Loss: 1.9341\nEpoch: 8/30... Step: 980... Loss: 2.0040... Val Loss: 1.9330\nEpoch: 8/30... Step: 990... Loss: 2.0095... Val Loss: 1.9299\nEpoch: 8/30... Step: 1000... Loss: 2.0032... Val Loss: 1.9236\nEpoch: 8/30... Step: 1010... Loss: 2.0209... Val Loss: 1.9209\nEpoch: 8/30... Step: 1020... Loss: 2.0040... Val Loss: 1.9184\nEpoch: 8/30... Step: 1030... Loss: 1.9807... Val Loss: 1.9188\nEpoch: 8/30... Step: 1040... Loss: 1.9876... Val Loss: 1.9112\nEpoch: 8/30... Step: 1050... Loss: 2.0014... Val Loss: 1.9076\nEpoch: 8/30... Step: 1060... Loss: 1.9896... Val Loss: 1.9023\nEpoch: 8/30... Step: 1070... Loss: 1.9983... Val Loss: 1.9005\nEpoch: 8/30... Step: 1080... Loss: 1.9747... Val Loss: 1.8984\nEpoch: 8/30... Step: 1090... Loss: 1.9656... Val Loss: 1.8956\nEpoch: 8/30... Step: 1100... Loss: 1.9669... Val Loss: 1.8923\nEpoch: 8/30... Step: 1110... Loss: 1.9742... Val Loss: 1.8880\nEpoch: 9/30... Step: 1120... Loss: 1.9827... Val Loss: 1.8874\nEpoch: 9/30... Step: 1130... Loss: 1.9644... Val Loss: 1.8820\nEpoch: 9/30... Step: 1140... Loss: 1.9803... Val Loss: 1.8781\nEpoch: 9/30... Step: 1150... Loss: 1.9827... Val Loss: 1.8775\nEpoch: 9/30... Step: 1160... Loss: 1.9381... Val Loss: 1.8770\nEpoch: 9/30... Step: 1170... Loss: 1.9418... Val Loss: 1.8725\nEpoch: 9/30... Step: 1180... Loss: 1.9439... Val Loss: 1.8673\nEpoch: 9/30... Step: 1190... Loss: 1.9792... Val Loss: 1.8631\nEpoch: 9/30... Step: 1200... Loss: 1.9287... Val Loss: 1.8601\nEpoch: 9/30... Step: 1210... Loss: 1.9303... Val Loss: 1.8578\nEpoch: 9/30... Step: 1220... Loss: 1.9242... Val Loss: 1.8560\nEpoch: 9/30... Step: 1230... Loss: 1.9174... Val Loss: 1.8538\nEpoch: 9/30... Step: 1240... Loss: 1.9193... Val Loss: 1.8501\nEpoch: 9/30... Step: 1250... Loss: 1.9223... Val Loss: 1.8497\nEpoch: 10/30... Step: 1260... Loss: 1.9297... Val Loss: 1.8456\nEpoch: 10/30... Step: 1270... Loss: 1.9231... Val Loss: 1.8445\nEpoch: 10/30... Step: 1280... Loss: 1.9394... Val Loss: 1.8413\nEpoch: 10/30... Step: 1290... Loss: 1.9295... Val Loss: 1.8406\nEpoch: 10/30... Step: 1300... Loss: 1.9196... Val Loss: 1.8394\nEpoch: 10/30... Step: 1310... Loss: 1.9334... Val Loss: 1.8340\nEpoch: 10/30... Step: 1320... Loss: 1.9171... Val Loss: 1.8308\nEpoch: 10/30... Step: 1330... Loss: 1.9167... Val Loss: 1.8278\nEpoch: 10/30... Step: 1340... Loss: 1.9075... Val Loss: 1.8234\nEpoch: 10/30... Step: 1350... Loss: 1.8947... Val Loss: 1.8253\nEpoch: 10/30... Step: 1360... Loss: 1.9104... Val Loss: 1.8215\nEpoch: 10/30... Step: 1370... Loss: 1.8925... Val Loss: 1.8204\nEpoch: 10/30... Step: 1380... Loss: 1.9163... Val Loss: 1.8160\nEpoch: 10/30... Step: 1390... Loss: 1.9060... Val Loss: 1.8179\nEpoch: 11/30... Step: 1400... Loss: 1.9142... Val Loss: 1.8149\nEpoch: 11/30... Step: 1410... Loss: 1.9036... Val Loss: 1.8125\nEpoch: 11/30... Step: 1420... Loss: 1.9113... Val Loss: 1.8066\nEpoch: 11/30... Step: 1430... Loss: 1.8727... Val Loss: 1.8045\nEpoch: 11/30... Step: 1440... Loss: 1.9356... Val Loss: 1.8019\nEpoch: 11/30... Step: 1450... Loss: 1.8648... Val Loss: 1.8011\nEpoch: 11/30... Step: 1460... Loss: 1.8749... Val Loss: 1.7982\nEpoch: 11/30... Step: 1470... Loss: 1.8650... Val Loss: 1.7969\nEpoch: 11/30... Step: 1480... Loss: 1.8882... Val Loss: 1.7946\nEpoch: 11/30... Step: 1490... Loss: 1.8789... Val Loss: 1.7933\nEpoch: 11/30... Step: 1500... Loss: 1.8703... Val Loss: 1.7918\nEpoch: 11/30... Step: 1510... Loss: 1.8412... Val Loss: 1.7890\nEpoch: 11/30... Step: 1520... Loss: 1.8861... Val Loss: 1.7912\nEpoch: 12/30... Step: 1530... Loss: 1.9047... Val Loss: 1.7870\nEpoch: 12/30... Step: 1540... Loss: 1.8856... Val Loss: 1.7841\nEpoch: 12/30... Step: 1550... Loss: 1.9029... Val Loss: 1.7797\nEpoch: 12/30... Step: 1560... Loss: 1.8833... Val Loss: 1.7772\nEpoch: 12/30... Step: 1570... Loss: 1.8584... Val Loss: 1.7758\nEpoch: 12/30... Step: 1580... Loss: 1.8215... Val Loss: 1.7745\nEpoch: 12/30... Step: 1590... Loss: 1.8366... Val Loss: 1.7726\nEpoch: 12/30... Step: 1600... Loss: 1.8655... Val Loss: 1.7712\nEpoch: 12/30... Step: 1610... Loss: 1.8373... Val Loss: 1.7691\nEpoch: 12/30... Step: 1620... Loss: 1.8634... Val Loss: 1.7676\nEpoch: 12/30... Step: 1630... Loss: 1.8657... Val Loss: 1.7661\nEpoch: 12/30... Step: 1640... Loss: 1.8438... Val Loss: 1.7650\nEpoch: 12/30... Step: 1650... Loss: 1.8193... Val Loss: 1.7615\nEpoch: 12/30... Step: 1660... Loss: 1.8650... Val Loss: 1.7623\nEpoch: 13/30... Step: 1670... Loss: 1.8645... Val Loss: 1.7603\nEpoch: 13/30... Step: 1680... Loss: 1.8584... Val Loss: 1.7576\nEpoch: 13/30... Step: 1690... Loss: 1.8289... Val Loss: 1.7529\nEpoch: 13/30... Step: 1700... Loss: 1.8465... Val Loss: 1.7529\nEpoch: 13/30... Step: 1710... Loss: 1.8207... Val Loss: 1.7530\nEpoch: 13/30... Step: 1720... Loss: 1.8122... Val Loss: 1.7503\nEpoch: 13/30... Step: 1730... Loss: 1.8606... Val Loss: 1.7478\nEpoch: 13/30... Step: 1740... Loss: 1.8188... Val Loss: 1.7474\nEpoch: 13/30... Step: 1750... Loss: 1.7961... Val Loss: 1.7467\nEpoch: 13/30... Step: 1760... Loss: 1.8390... Val Loss: 1.7447\nEpoch: 13/30... Step: 1770... Loss: 1.8401... Val Loss: 1.7432\nEpoch: 13/30... Step: 1780... Loss: 1.8230... Val Loss: 1.7411\nEpoch: 13/30... Step: 1790... Loss: 1.7895... Val Loss: 1.7411\nEpoch: 13/30... Step: 1800... Loss: 1.8372... Val Loss: 1.7422\nEpoch: 14/30... Step: 1810... Loss: 1.8278... Val Loss: 1.7392\nEpoch: 14/30... Step: 1820... Loss: 1.8260... Val Loss: 1.7347\nEpoch: 14/30... Step: 1830... Loss: 1.8294... Val Loss: 1.7306\nEpoch: 14/30... Step: 1840... Loss: 1.7818... Val Loss: 1.7307\nEpoch: 14/30... Step: 1850... Loss: 1.7772... Val Loss: 1.7324\nEpoch: 14/30... Step: 1860... Loss: 1.8277... Val Loss: 1.7293\nEpoch: 14/30... Step: 1870... Loss: 1.8166... Val Loss: 1.7266\nEpoch: 14/30... Step: 1880... Loss: 1.8261... Val Loss: 1.7255\nEpoch: 14/30... Step: 1890... Loss: 1.8315... Val Loss: 1.7244\nEpoch: 14/30... Step: 1900... Loss: 1.8061... Val Loss: 1.7229\nEpoch: 14/30... Step: 1910... Loss: 1.8365... Val Loss: 1.7209\nEpoch: 14/30... Step: 1920... Loss: 1.7970... Val Loss: 1.7214\nEpoch: 14/30... Step: 1930... Loss: 1.7747... Val Loss: 1.7203\nEpoch: 14/30... Step: 1940... Loss: 1.8217... Val Loss: 1.7205\nEpoch: 15/30... Step: 1950... Loss: 1.8055... Val Loss: 1.7189\nEpoch: 15/30... Step: 1960... Loss: 1.8000... Val Loss: 1.7149\nEpoch: 15/30... Step: 1970... Loss: 1.7908... Val Loss: 1.7130\nEpoch: 15/30... Step: 1980... Loss: 1.7815... Val Loss: 1.7099\nEpoch: 15/30... Step: 1990... Loss: 1.7905... Val Loss: 1.7124\nEpoch: 15/30... Step: 2000... Loss: 1.7810... Val Loss: 1.7098\nEpoch: 15/30... Step: 2010... Loss: 1.7884... Val Loss: 1.7096\nEpoch: 15/30... Step: 2020... Loss: 1.8204... Val Loss: 1.7062\nEpoch: 15/30... Step: 2030... Loss: 1.7901... Val Loss: 1.7060\nEpoch: 15/30... Step: 2040... Loss: 1.7738... Val Loss: 1.7041\nEpoch: 15/30... Step: 2050... Loss: 1.7692... Val Loss: 1.7042\nEpoch: 15/30... Step: 2060... Loss: 1.7918... Val Loss: 1.7041\nEpoch: 15/30... Step: 2070... Loss: 1.7845... Val Loss: 1.7031\nEpoch: 15/30... Step: 2080... Loss: 1.7815... Val Loss: 1.7008\nEpoch: 16/30... Step: 2090... Loss: 1.7861... Val Loss: 1.7009\nEpoch: 16/30... Step: 2100... Loss: 1.7769... Val Loss: 1.6995\nEpoch: 16/30... Step: 2110... Loss: 1.7694... Val Loss: 1.7012\nEpoch: 16/30... Step: 2120... Loss: 1.7874... Val Loss: 1.6950\nEpoch: 16/30... Step: 2130... Loss: 1.7588... Val Loss: 1.6957\nEpoch: 16/30... Step: 2140... Loss: 1.7465... Val Loss: 1.6926\nEpoch: 16/30... Step: 2150... Loss: 1.7790... Val Loss: 1.6934\nEpoch: 16/30... Step: 2160... Loss: 1.7744... Val Loss: 1.6955\nEpoch: 16/30... Step: 2170... Loss: 1.7693... Val Loss: 1.6916\nEpoch: 16/30... Step: 2180... Loss: 1.7591... Val Loss: 1.6874\nEpoch: 16/30... Step: 2190... Loss: 1.7731... Val Loss: 1.6865\nEpoch: 16/30... Step: 2200... Loss: 1.7723... Val Loss: 1.6893\nEpoch: 16/30... Step: 2210... Loss: 1.7381... Val Loss: 1.6880\nEpoch: 16/30... Step: 2220... Loss: 1.7753... Val Loss: 1.6859\nEpoch: 17/30... Step: 2230... Loss: 1.7341... Val Loss: 1.6851\nEpoch: 17/30... Step: 2240... Loss: 1.7535... Val Loss: 1.6846\nEpoch: 17/30... Step: 2250... Loss: 1.7433... Val Loss: 1.6864\nEpoch: 17/30... Step: 2260... Loss: 1.7674... Val Loss: 1.6813\nEpoch: 17/30... Step: 2270... Loss: 1.7659... Val Loss: 1.6805\nEpoch: 17/30... Step: 2280... Loss: 1.7606... Val Loss: 1.6791\nEpoch: 17/30... Step: 2290... Loss: 1.7453... Val Loss: 1.6782\nEpoch: 17/30... Step: 2300... Loss: 1.7254... Val Loss: 1.6801\nEpoch: 17/30... Step: 2310... Loss: 1.7444... Val Loss: 1.6787\nEpoch: 17/30... Step: 2320... Loss: 1.7446... Val Loss: 1.6746\nEpoch: 17/30... Step: 2330... Loss: 1.7480... Val Loss: 1.6745\nEpoch: 17/30... Step: 2340... Loss: 1.7610... Val Loss: 1.6787\nEpoch: 17/30... Step: 2350... Loss: 1.7703... Val Loss: 1.6761\nEpoch: 17/30... Step: 2360... Loss: 1.7697... Val Loss: 1.6733\nEpoch: 18/30... Step: 2370... Loss: 1.7522... Val Loss: 1.6734\nEpoch: 18/30... Step: 2380... Loss: 1.7520... Val Loss: 1.6695\nEpoch: 18/30... Step: 2390... Loss: 1.7457... Val Loss: 1.6704\nEpoch: 18/30... Step: 2400... Loss: 1.7824... Val Loss: 1.6704\nEpoch: 18/30... Step: 2410... Loss: 1.7516... Val Loss: 1.6678\nEpoch: 18/30... Step: 2420... Loss: 1.7443... Val Loss: 1.6655\nEpoch: 18/30... Step: 2430... Loss: 1.7456... Val Loss: 1.6666\nEpoch: 18/30... Step: 2440... Loss: 1.7457... Val Loss: 1.6666\nEpoch: 18/30... Step: 2450... Loss: 1.7414... Val Loss: 1.6637\nEpoch: 18/30... Step: 2460... Loss: 1.7648... Val Loss: 1.6610\nEpoch: 18/30... Step: 2470... Loss: 1.7401... Val Loss: 1.6623\nEpoch: 18/30... Step: 2480... Loss: 1.7328... Val Loss: 1.6637\nEpoch: 18/30... Step: 2490... Loss: 1.7343... Val Loss: 1.6614\nEpoch: 18/30... Step: 2500... Loss: 1.7430... Val Loss: 1.6594\nEpoch: 19/30... Step: 2510... Loss: 1.7596... Val Loss: 1.6594\nEpoch: 19/30... Step: 2520... Loss: 1.7504... Val Loss: 1.6556\nEpoch: 19/30... Step: 2530... Loss: 1.7456... Val Loss: 1.6543\nEpoch: 19/30... Step: 2540... Loss: 1.7575... Val Loss: 1.6575\nEpoch: 19/30... Step: 2550... Loss: 1.7144... Val Loss: 1.6549\nEpoch: 19/30... Step: 2560... Loss: 1.7222... Val Loss: 1.6548\nEpoch: 19/30... Step: 2570... Loss: 1.7175... Val Loss: 1.6522\nEpoch: 19/30... Step: 2580... Loss: 1.7561... Val Loss: 1.6505\nEpoch: 19/30... Step: 2590... Loss: 1.7118... Val Loss: 1.6533\nEpoch: 19/30... Step: 2600... Loss: 1.7242... Val Loss: 1.6499\nEpoch: 19/30... Step: 2610... Loss: 1.7186... Val Loss: 1.6494\nEpoch: 19/30... Step: 2620... Loss: 1.7177... Val Loss: 1.6498\nEpoch: 19/30... Step: 2630... Loss: 1.7095... Val Loss: 1.6502\nEpoch: 19/30... Step: 2640... Loss: 1.7217... Val Loss: 1.6544\nEpoch: 20/30... Step: 2650... Loss: 1.7412... Val Loss: 1.6496\nEpoch: 20/30... Step: 2660... Loss: 1.7251... Val Loss: 1.6461\nEpoch: 20/30... Step: 2670... Loss: 1.7468... Val Loss: 1.6443\nEpoch: 20/30... Step: 2680... Loss: 1.7355... Val Loss: 1.6463\nEpoch: 20/30... Step: 2690... Loss: 1.7175... Val Loss: 1.6442\nEpoch: 20/30... Step: 2700... Loss: 1.7331... Val Loss: 1.6418\nEpoch: 20/30... Step: 2710... Loss: 1.7081... Val Loss: 1.6400\nEpoch: 20/30... Step: 2720... Loss: 1.7159... Val Loss: 1.6402\nEpoch: 20/30... Step: 2730... Loss: 1.7126... Val Loss: 1.6400\nEpoch: 20/30... Step: 2740... Loss: 1.7015... Val Loss: 1.6386\nEpoch: 20/30... Step: 2750... Loss: 1.7140... Val Loss: 1.6378\nEpoch: 20/30... Step: 2760... Loss: 1.7022... Val Loss: 1.6403\nEpoch: 20/30... Step: 2770... Loss: 1.7358... Val Loss: 1.6379\nEpoch: 20/30... Step: 2780... Loss: 1.7450... Val Loss: 1.6400\nEpoch: 21/30... Step: 2790... Loss: 1.7317... Val Loss: 1.6364\nEpoch: 21/30... Step: 2800... Loss: 1.7379... Val Loss: 1.6347\nEpoch: 21/30... Step: 2810... Loss: 1.7321... Val Loss: 1.6332\nEpoch: 21/30... Step: 2820... Loss: 1.7143... Val Loss: 1.6378\nEpoch: 21/30... Step: 2830... Loss: 1.7547... Val Loss: 1.6330\nEpoch: 21/30... Step: 2840... Loss: 1.6779... Val Loss: 1.6323\nEpoch: 21/30... Step: 2850... Loss: 1.6999... Val Loss: 1.6324\nEpoch: 21/30... Step: 2860... Loss: 1.6955... Val Loss: 1.6299\nEpoch: 21/30... Step: 2870... Loss: 1.7188... Val Loss: 1.6310\nEpoch: 21/30... Step: 2880... Loss: 1.7064... Val Loss: 1.6297\nEpoch: 21/30... Step: 2890... Loss: 1.6867... Val Loss: 1.6302\nEpoch: 21/30... Step: 2900... Loss: 1.6819... Val Loss: 1.6314\nEpoch: 21/30... Step: 2910... Loss: 1.7256... Val Loss: 1.6279\nEpoch: 22/30... Step: 2920... Loss: 1.7445... Val Loss: 1.6291\nEpoch: 22/30... Step: 2930... Loss: 1.7241... Val Loss: 1.6286\nEpoch: 22/30... Step: 2940... Loss: 1.7475... Val Loss: 1.6278\nEpoch: 22/30... Step: 2950... Loss: 1.7356... Val Loss: 1.6251\nEpoch: 22/30... Step: 2960... Loss: 1.7032... Val Loss: 1.6251\nEpoch: 22/30... Step: 2970... Loss: 1.6652... Val Loss: 1.6263\nEpoch: 22/30... Step: 2980... Loss: 1.6652... Val Loss: 1.6245\nEpoch: 22/30... Step: 2990... Loss: 1.6953... Val Loss: 1.6244\nEpoch: 22/30... Step: 3000... Loss: 1.6860... Val Loss: 1.6213\nEpoch: 22/30... Step: 3010... Loss: 1.7004... Val Loss: 1.6199\nEpoch: 22/30... Step: 3020... Loss: 1.7194... Val Loss: 1.6206\nEpoch: 22/30... Step: 3030... Loss: 1.6890... Val Loss: 1.6219\nEpoch: 22/30... Step: 3040... Loss: 1.6667... Val Loss: 1.6210\nEpoch: 22/30... Step: 3050... Loss: 1.7187... Val Loss: 1.6190\nEpoch: 23/30... Step: 3060... Loss: 1.6982... Val Loss: 1.6213\nEpoch: 23/30... Step: 3070... Loss: 1.7012... Val Loss: 1.6178\nEpoch: 23/30... Step: 3080... Loss: 1.6785... Val Loss: 1.6208\nEpoch: 23/30... Step: 3090... Loss: 1.6943... Val Loss: 1.6156\nEpoch: 23/30... Step: 3100... Loss: 1.6602... Val Loss: 1.6148\nEpoch: 23/30... Step: 3110... Loss: 1.6805... Val Loss: 1.6164\nEpoch: 23/30... Step: 3120... Loss: 1.7115... Val Loss: 1.6112\nEpoch: 23/30... Step: 3130... Loss: 1.6813... Val Loss: 1.6157\nEpoch: 23/30... Step: 3140... Loss: 1.6500... Val Loss: 1.6142\nEpoch: 23/30... Step: 3150... Loss: 1.6864... Val Loss: 1.6114\nEpoch: 23/30... Step: 3160... Loss: 1.6940... Val Loss: 1.6147\nEpoch: 23/30... Step: 3170... Loss: 1.6758... Val Loss: 1.6120\nEpoch: 23/30... Step: 3180... Loss: 1.6639... Val Loss: 1.6127\nEpoch: 23/30... Step: 3190... Loss: 1.6970... Val Loss: 1.6107\nEpoch: 24/30... Step: 3200... Loss: 1.7015... Val Loss: 1.6134\nEpoch: 24/30... Step: 3210... Loss: 1.6944... Val Loss: 1.6092\nEpoch: 24/30... Step: 3220... Loss: 1.6896... Val Loss: 1.6108\nEpoch: 24/30... Step: 3230... Loss: 1.6432... Val Loss: 1.6100\nEpoch: 24/30... Step: 3240... Loss: 1.6348... Val Loss: 1.6058\nEpoch: 24/30... Step: 3250... Loss: 1.6942... Val Loss: 1.6071\nEpoch: 24/30... Step: 3260... Loss: 1.6824... Val Loss: 1.6058\nEpoch: 24/30... Step: 3270... Loss: 1.6941... Val Loss: 1.6057\nEpoch: 24/30... Step: 3280... Loss: 1.6979... Val Loss: 1.6093\nEpoch: 24/30... Step: 3290... Loss: 1.6870... Val Loss: 1.6024\nEpoch: 24/30... Step: 3300... Loss: 1.7001... Val Loss: 1.6046\nEpoch: 24/30... Step: 3310... Loss: 1.6790... Val Loss: 1.6029\nEpoch: 24/30... Step: 3320... Loss: 1.6477... Val Loss: 1.6048\nEpoch: 24/30... Step: 3330... Loss: 1.7007... Val Loss: 1.6034\nEpoch: 25/30... Step: 3340... Loss: 1.6721... Val Loss: 1.6068\nEpoch: 25/30... Step: 3350... Loss: 1.6815... Val Loss: 1.6028\nEpoch: 25/30... Step: 3360... Loss: 1.6590... Val Loss: 1.6001\nEpoch: 25/30... Step: 3370... Loss: 1.6671... Val Loss: 1.6017\nEpoch: 25/30... Step: 3380... Loss: 1.6630... Val Loss: 1.6004\nEpoch: 25/30... Step: 3390... Loss: 1.6607... Val Loss: 1.5990\nEpoch: 25/30... Step: 3400... Loss: 1.6640... Val Loss: 1.5997\nEpoch: 25/30... Step: 3410... Loss: 1.6958... Val Loss: 1.5961\nEpoch: 25/30... Step: 3420... Loss: 1.6572... Val Loss: 1.6000\nEpoch: 25/30... Step: 3430... Loss: 1.6674... Val Loss: 1.5946\nEpoch: 25/30... Step: 3440... Loss: 1.6400... Val Loss: 1.5955\nEpoch: 25/30... Step: 3450... Loss: 1.6777... Val Loss: 1.5956\nEpoch: 25/30... Step: 3460... Loss: 1.6834... Val Loss: 1.5965\nEpoch: 25/30... Step: 3470... Loss: 1.6687... Val Loss: 1.5958\nEpoch: 26/30... Step: 3480... Loss: 1.6693... Val Loss: 1.6001\nEpoch: 26/30... Step: 3490... Loss: 1.6709... Val Loss: 1.5973\nEpoch: 26/30... Step: 3500... Loss: 1.6569... Val Loss: 1.5939\nEpoch: 26/30... Step: 3510... Loss: 1.6781... Val Loss: 1.5937\nEpoch: 26/30... Step: 3520... Loss: 1.6445... Val Loss: 1.5936\nEpoch: 26/30... Step: 3530... Loss: 1.6297... Val Loss: 1.5909\nEpoch: 26/30... Step: 3540... Loss: 1.6755... Val Loss: 1.5947\nEpoch: 26/30... Step: 3550... Loss: 1.6627... Val Loss: 1.5894\nEpoch: 26/30... Step: 3560... Loss: 1.6525... Val Loss: 1.5944\nEpoch: 26/30... Step: 3570... Loss: 1.6485... Val Loss: 1.5866\nEpoch: 26/30... Step: 3580... Loss: 1.6615... Val Loss: 1.5885\nEpoch: 26/30... Step: 3590... Loss: 1.6639... Val Loss: 1.5910\nEpoch: 26/30... Step: 3600... Loss: 1.6306... Val Loss: 1.5884\nEpoch: 26/30... Step: 3610... Loss: 1.6700... Val Loss: 1.5900\nEpoch: 27/30... Step: 3620... Loss: 1.6286... Val Loss: 1.5927\nEpoch: 27/30... Step: 3630... Loss: 1.6595... Val Loss: 1.5884\nEpoch: 27/30... Step: 3640... Loss: 1.6529... Val Loss: 1.5879\nEpoch: 27/30... Step: 3650... Loss: 1.6551... Val Loss: 1.5857\nEpoch: 27/30... Step: 3660... Loss: 1.6581... Val Loss: 1.5885\nEpoch: 27/30... Step: 3670... Loss: 1.6494... Val Loss: 1.5830\nEpoch: 27/30... Step: 3680... Loss: 1.6493... Val Loss: 1.5864\nEpoch: 27/30... Step: 3690... Loss: 1.6177... Val Loss: 1.5823\nEpoch: 27/30... Step: 3700... Loss: 1.6368... Val Loss: 1.5846\nEpoch: 27/30... Step: 3710... Loss: 1.6368... Val Loss: 1.5826\nEpoch: 27/30... Step: 3720... Loss: 1.6424... Val Loss: 1.5809\nEpoch: 27/30... Step: 3730... Loss: 1.6511... Val Loss: 1.5840\nEpoch: 27/30... Step: 3740... Loss: 1.6651... Val Loss: 1.5802\nEpoch: 27/30... Step: 3750... Loss: 1.6708... Val Loss: 1.5799\nEpoch: 28/30... Step: 3760... Loss: 1.6454... Val Loss: 1.5840\nEpoch: 28/30... Step: 3770... Loss: 1.6528... Val Loss: 1.5779\nEpoch: 28/30... Step: 3780... Loss: 1.6512... Val Loss: 1.5794\nEpoch: 28/30... Step: 3790... Loss: 1.6828... Val Loss: 1.5781\nEpoch: 28/30... Step: 3800... Loss: 1.6646... Val Loss: 1.5793\nEpoch: 28/30... Step: 3810... Loss: 1.6454... Val Loss: 1.5761\nEpoch: 28/30... Step: 3820... Loss: 1.6543... Val Loss: 1.5794\nEpoch: 28/30... Step: 3830... Loss: 1.6396... Val Loss: 1.5769\nEpoch: 28/30... Step: 3840... Loss: 1.6339... Val Loss: 1.5736\nEpoch: 28/30... Step: 3850... Loss: 1.6660... Val Loss: 1.5765\nEpoch: 28/30... Step: 3860... Loss: 1.6544... Val Loss: 1.5739\nEpoch: 28/30... Step: 3870... Loss: 1.6462... Val Loss: 1.5778\nEpoch: 28/30... Step: 3880... Loss: 1.6335... Val Loss: 1.5747\nEpoch: 28/30... Step: 3890... Loss: 1.6491... Val Loss: 1.5747\nEpoch: 29/30... Step: 3900... Loss: 1.6621... Val Loss: 1.5807\nEpoch: 29/30... Step: 3910... Loss: 1.6552... Val Loss: 1.5722\nEpoch: 29/30... Step: 3920... Loss: 1.6632... Val Loss: 1.5726\nEpoch: 29/30... Step: 3930... Loss: 1.6667... Val Loss: 1.5721\nEpoch: 29/30... Step: 3940... Loss: 1.6315... Val Loss: 1.5709\nEpoch: 29/30... Step: 3950... Loss: 1.6359... Val Loss: 1.5724\nEpoch: 29/30... Step: 3960... Loss: 1.6312... Val Loss: 1.5710\nEpoch: 29/30... Step: 3970... Loss: 1.6686... Val Loss: 1.5715\nEpoch: 29/30... Step: 3980... Loss: 1.6190... Val Loss: 1.5687\nEpoch: 29/30... Step: 3990... Loss: 1.6316... Val Loss: 1.5694\nEpoch: 29/30... Step: 4000... Loss: 1.6322... Val Loss: 1.5684\nEpoch: 29/30... Step: 4010... Loss: 1.6267... Val Loss: 1.5711\nEpoch: 29/30... Step: 4020... Loss: 1.6272... Val Loss: 1.5693\nEpoch: 29/30... Step: 4030... Loss: 1.6419... Val Loss: 1.5691\nEpoch: 30/30... Step: 4040... Loss: 1.6478... Val Loss: 1.5736\nEpoch: 30/30... Step: 4050... Loss: 1.6451... Val Loss: 1.5683\nEpoch: 30/30... Step: 4060... Loss: 1.6528... Val Loss: 1.5665\nEpoch: 30/30... Step: 4070... Loss: 1.6435... Val Loss: 1.5680\nEpoch: 30/30... Step: 4080... Loss: 1.6358... Val Loss: 1.5655\nEpoch: 30/30... Step: 4090... Loss: 1.6421... Val Loss: 1.5679\nEpoch: 30/30... Step: 4100... Loss: 1.6150... Val Loss: 1.5649\nEpoch: 30/30... Step: 4110... Loss: 1.6276... Val Loss: 1.5669\nEpoch: 30/30... Step: 4120... Loss: 1.6179... Val Loss: 1.5635\nEpoch: 30/30... Step: 4130... Loss: 1.6222... Val Loss: 1.5665\nEpoch: 30/30... Step: 4140... Loss: 1.6174... Val Loss: 1.5628\nEpoch: 30/30... Step: 4150... Loss: 1.6082... Val Loss: 1.5650\nEpoch: 30/30... Step: 4160... Loss: 1.6399... Val Loss: 1.5618\nEpoch: 30/30... Step: 4170... Loss: 1.6616... Val Loss: 1.5619\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Tips and Tricks\nMonitoring Validation Loss vs. Training Loss\nIf you're somewhat new to Machine Learning or Neural Networks it can take a bit of expertise to get good models. The most important quantity to keep track of is the difference between your training loss (printed during training) and the validation loss (printed once in a while when the RNN is run on the validation data (by default every 1000 iterations)). In particular:\n\nIf your training loss is much lower than validation loss then this means the network might be overfitting. Solutions to this are to decrease your network size, or to increase dropout. For example you could try dropout of 0.5 and so on.\nIf your training/validation loss are about equal then your model is underfitting. Increase the size of your model (either number of layers or the raw number of neurons per layer)\nApproximate number of parameters\nThe two most important parameters that control the model are n_hidden and n_layers. I would advise that you always use n_layers of either 2/3. The n_hidden can be adjusted based on how much data you have. The two important quantities to keep track of here are:\n\nThe number of parameters in your model. This is printed when you start training.\nThe size of your dataset. 1MB file is approximately 1 million characters.\nThese two should be about the same order of magnitude. It's a little tricky to tell. Here are some examples:\n\nI have a 100MB dataset and I'm using the default parameter settings (which currently print 150K parameters). My data size is significantly larger (100 mil >> 0.15 mil), so I expect to heavily underfit. I am thinking I can comfortably afford to make n_hidden larger.\nI have a 10MB dataset and running a 10 million parameter model. I'm slightly nervous and I'm carefully monitoring my validation loss. If it's larger than my training loss then I may want to try to increase dropout a bit and see if that helps the validation loss.\nBest models strategy\nThe winning strategy to obtaining very good models (if you have the compute time) is to always err on making the network larger (as large as you're willing to wait for it to compute) and then try different dropout values (between 0,1). Whatever model has the best validation performance (the loss, written in the checkpoint filename, low is good) is the one you should use in the end.\n\nIt is very common in deep learning to run many different models with many different hyperparameter settings, and in the end take whatever checkpoint gave the best validation performance.\n\nBy the way, the size of your training and validation splits are also parameters. Make sure you have a decent amount of data in your validation set or otherwise the validation performance will be noisy and not very informative.","metadata":{}},{"cell_type":"code","source":"#checkpoint\nmodel_name = 'rnn_30_epoch.net'\ncheckpoint ={'n_hidden':net.n_hidden,\n            'n_layers':net.n_layers,\n            'state_dict':net.state_dict(),\n            'tokens':net.chars}\nwith open(model_name,'wb') as f:\n    torch.save(checkpoint,f)","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:19:53.868615Z","iopub.execute_input":"2021-05-24T22:19:53.868948Z","iopub.status.idle":"2021-05-24T22:19:53.878180Z","shell.execute_reply.started":"2021-05-24T22:19:53.868915Z","shell.execute_reply":"2021-05-24T22:19:53.877339Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# Prediction\n# Top K sampling\nOur predictions come from a categorical probability distribution over all the possible characters. We can make the sample text and make it more reasonable to handle (with less variables) by only considering some $K$ most probable characters. This will prevent the network from giving us completely absurd characters while allowing it to introduce some noise and randomness into the sampled text. Read more about topk, here.","metadata":{}},{"cell_type":"code","source":"def predict(net, char, h=None, top_k=None):\n    #tensor inputs\n    x=np.array([[net.char2int[char]]])\n    x=one_hot_encode(x,len(net.chars))\n    inputs = torch.from_numpy(x)\n    \n    if(train_on_gpu):\n        inputs=inputs.cuda()\n    h=tuple([each.data for each in h])\n    out,h=net(inputs,h)\n    \n    p=F.softmax(out,dim=1).data\n    if(train_on_gpu):\n        p=p.cpu()\n    #get top characters\n    if top_k is None:\n        top_ch=np.arange(len(net.chars))\n    else:\n        p,top_ch=p.topk(top_k)\n        top_ch=top_ch.numpy().squeeze()\n        \n    p=p.numpy().squeeze()\n    char=np.random.choice(top_ch,p=p/p.sum())\n    return net.int2char[char], h\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:20:19.804476Z","iopub.execute_input":"2021-05-24T22:20:19.804789Z","iopub.status.idle":"2021-05-24T22:20:19.814051Z","shell.execute_reply.started":"2021-05-24T22:20:19.804759Z","shell.execute_reply":"2021-05-24T22:20:19.812896Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def sample(net, size, prime='The', top_k=None):\n        \n    if(train_on_gpu):\n        net.cuda()\n    else:\n        net.cpu()\n    \n    net.eval() # eval mode\n    \n    # First off, run through the prime characters\n    chars = [ch for ch in prime]\n    h = net.init_hidden(1)\n    for ch in prime:\n        char, h = predict(net, ch, h, top_k=top_k)\n\n    chars.append(char)\n    \n    # Now pass in the previous character and get a new one\n    for ii in range(size):\n        char, h = predict(net, chars[-1], h, top_k=top_k)\n        chars.append(char)\n\n    return ''.join(chars)","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:20:23.084135Z","iopub.execute_input":"2021-05-24T22:20:23.084468Z","iopub.status.idle":"2021-05-24T22:20:23.090936Z","shell.execute_reply.started":"2021-05-24T22:20:23.084437Z","shell.execute_reply":"2021-05-24T22:20:23.089794Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"print(sample(net, 1000, prime='Anna', top_k=5))","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:24:09.710306Z","iopub.execute_input":"2021-05-24T22:24:09.710634Z","iopub.status.idle":"2021-05-24T22:24:10.309404Z","shell.execute_reply.started":"2021-05-24T22:24:09.710603Z","shell.execute_reply":"2021-05-24T22:24:10.308537Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Anna, she's not, something to mise as the care and a master to them which he to did not a seas of\nher.\"\n\n\"Time,\" Anna he was not tore the poins\nand an offered to the coming and were the mindinate along his\nconsearing\ntime, said\nher at the chanting. Her face, and\nthe betingeres,\nand and himself wourded it with her\nhand, without thought and had such\nan angering their stard were were attining on the same-deat were his\nsenters and had been had nothing\nsteant in the proness tower the shalled, betanes of her that he to hore as in a day the corting of this shame to him had\ntellent time of the praiced her hands, says that and he had a beto one.\n\n\"I don't sole that that so though always some and think anywhinc of her all have here to her a sayed, as the seepe ther and as that any seem of the princess that that she would not have nother intayed out of the part a talk, better,\nbut all her to her.\n\"I'll to me work. I he what there was began all sighted\nher this seemed\nto the manding. How seem, sam the \n","output_type":"stream"}]},{"cell_type":"code","source":"# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\nwith open('rnn_30_epoch.net', 'rb') as f:\n    checkpoint = torch.load(f)\n    \nloaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\nloaded.load_state_dict(checkpoint['state_dict'])","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:20:33.995338Z","iopub.execute_input":"2021-05-24T22:20:33.995660Z","iopub.status.idle":"2021-05-24T22:20:34.013987Z","shell.execute_reply.started":"2021-05-24T22:20:33.995629Z","shell.execute_reply":"2021-05-24T22:20:34.013230Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"\n# Sample using a loaded model\nprint(sample(loaded, 2000, top_k=5, prime=\"So\"))","metadata":{"execution":{"iopub.status.busy":"2021-05-24T22:24:45.480192Z","iopub.execute_input":"2021-05-24T22:24:45.480506Z","iopub.status.idle":"2021-05-24T22:24:46.768743Z","shell.execute_reply.started":"2021-05-24T22:24:45.480476Z","shell.execute_reply":"2021-05-24T22:24:46.767767Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Sorsky. I'm take herself.,\" said he had not back to her frow a both a forering. Stepan Arkadyevitch, would homtent to\nher all him. She had seen to\nhis\nconsidention, with\nhim, something and that still on the saw on his tears satting the biss and had and so wathing of\nhis husbands at him, what was the poosed to his thee him were all of astiin an her are to the manse and had to the thee fored, towards the charses and shome her that so was a shound, but the carting. He was so whole and that the sensing thought and hid houre he was a lecting of the steply was not\nas a drouse, she was had seeping the more when\nhis count that and with the counted, hander. \"What's a listered this. I soun time, and a lively. I stall to be there's been anyone time and that say.\"\n\nHe came his wife of the starl on his and with she had tanted her heart of the boughts and to hore the son heart. He were her heast and her sate, and\nso happened his was, as that alexing himself of insate took and a lift who had brought of something all his has, strong of it a talked of the best horrince with his\ntome would shall with work to the poinss, and a sharred it was\nshe stail at and the\nceararily\nof the both to she was homing. He taken her sometion, while the sister at which, that that seemed his to betone at the\nportor at the merto to her hand. \"Why seed a change,\" she said, and stopt her stow that at having and say he canetty at\nong of the prates of the crount of the\nshound of the\nmore who was not that she\nwas to be sat to her fout of streng on the saw to the proper would be seen at the\ncame him a corvinced to the meart and he shate of the momening when he said in the said and horsition\nhis with and shaned and well, all the sont\nas she what she seemed her which\nwhich who had been think and said.\n\n\"Yes, I would not have that them,\" said Alexey\nAlexandrovitch would not see she was attented at his soming and would the to aser the same\nall shortire, seening. She was astacion to him the\nmeating\nthere, and the stoor\n","output_type":"stream"}]}]}